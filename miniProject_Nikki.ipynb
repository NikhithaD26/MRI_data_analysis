{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "045bbdee-11dd-47d0-b47c-2e9f31061de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels pygam --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2f7b791-6a8c-41f4-bf8a-8b4467c70b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RBCPath('rbc://PNC_FreeSurfer//home/jovyan/shared/data/RBC/PNC_FreeSurfer/freesurfer/sub-1000393599/sub-1000393599_brainmeasures.json')\n",
      "RBCPath('rbc://PNC_FreeSurfer//home/jovyan/shared/data/RBC/PNC_FreeSurfer/freesurfer/sub-1000393599/sub-1000393599_brainmeasures.tsv')\n",
      "RBCPath('rbc://PNC_FreeSurfer//home/jovyan/shared/data/RBC/PNC_FreeSurfer/freesurfer/sub-1000393599/sub-1000393599_freesurfer.tar.xz')\n",
      "RBCPath('rbc://PNC_FreeSurfer//home/jovyan/shared/data/RBC/PNC_FreeSurfer/freesurfer/sub-1000393599/sub-1000393599_fsLR_den-164k.tar.xz')\n",
      "RBCPath('rbc://PNC_FreeSurfer//home/jovyan/shared/data/RBC/PNC_FreeSurfer/freesurfer/sub-1000393599/sub-1000393599_fsaverage.tar.xz')\n",
      "RBCPath('rbc://PNC_FreeSurfer//home/jovyan/shared/data/RBC/PNC_FreeSurfer/freesurfer/sub-1000393599/sub-1000393599_regionsurfacestats.tsv')\n",
      "Loading rbc://PNC_FreeSurfer/freesurfer/sub-1000393599/sub-1000393599_regionsurfacestats.tsv ...\n",
      "Loading surface areas...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336c70883f084e8f9626d49a6df1b9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=1601)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression:\n",
      "  Intercept: 0.42976633990916446\n",
      "  Slope: [-0.0003118]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>study</th>\n",
       "      <th>study_site</th>\n",
       "      <th>session_id</th>\n",
       "      <th>wave</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>race</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>bmi</th>\n",
       "      <th>handedness</th>\n",
       "      <th>participant_education</th>\n",
       "      <th>parent_1_education</th>\n",
       "      <th>parent_2_education</th>\n",
       "      <th>p_factor</th>\n",
       "      <th>internalizing_mcelroy_harmonized_all_samples</th>\n",
       "      <th>externalizing_mcelroy_harmonized_all_samples</th>\n",
       "      <th>attention_mcelroy_harmonized_all_samples</th>\n",
       "      <th>cubids_acquisition_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000881804</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.916667</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>not Hispanic or Latino</td>\n",
       "      <td>21.52</td>\n",
       "      <td>Right</td>\n",
       "      <td>7th Grade</td>\n",
       "      <td>Complete secondary</td>\n",
       "      <td>Complete secondary</td>\n",
       "      <td>-0.429863</td>\n",
       "      <td>0.097355</td>\n",
       "      <td>0.387355</td>\n",
       "      <td>-0.467807</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100527940</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.250000</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>not Hispanic or Latino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ambidextrous</td>\n",
       "      <td>1st Grade</td>\n",
       "      <td>Complete secondary</td>\n",
       "      <td>Complete primary</td>\n",
       "      <td>-0.416456</td>\n",
       "      <td>0.699062</td>\n",
       "      <td>-0.781881</td>\n",
       "      <td>-0.982040</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1006151876</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>Female</td>\n",
       "      <td>Other</td>\n",
       "      <td>not Hispanic or Latino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Right</td>\n",
       "      <td>12th Grade</td>\n",
       "      <td>Complete tertiary</td>\n",
       "      <td>Complete secondary</td>\n",
       "      <td>-0.309821</td>\n",
       "      <td>0.495947</td>\n",
       "      <td>0.806481</td>\n",
       "      <td>-0.832210</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1012530688</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.750000</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>not Hispanic or Latino</td>\n",
       "      <td>21.36</td>\n",
       "      <td>Right</td>\n",
       "      <td>2nd Grade</td>\n",
       "      <td>Complete tertiary</td>\n",
       "      <td>Complete secondary</td>\n",
       "      <td>-0.666207</td>\n",
       "      <td>-0.334835</td>\n",
       "      <td>1.277773</td>\n",
       "      <td>0.161110</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1030193285</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>not Hispanic or Latino</td>\n",
       "      <td>22.15</td>\n",
       "      <td>Right</td>\n",
       "      <td>10th Grade</td>\n",
       "      <td>Complete secondary</td>\n",
       "      <td>Complete primary</td>\n",
       "      <td>-0.292984</td>\n",
       "      <td>1.027404</td>\n",
       "      <td>-0.490472</td>\n",
       "      <td>2.014568</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>969649154</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>not Hispanic or Latino</td>\n",
       "      <td>17.38</td>\n",
       "      <td>Right</td>\n",
       "      <td>5th Grade</td>\n",
       "      <td>Complete tertiary</td>\n",
       "      <td>Complete secondary</td>\n",
       "      <td>-0.532445</td>\n",
       "      <td>-0.148520</td>\n",
       "      <td>0.556444</td>\n",
       "      <td>0.024228</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>970890500</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>18.166667</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>not Hispanic or Latino</td>\n",
       "      <td>30.89</td>\n",
       "      <td>Right</td>\n",
       "      <td>11th Grade</td>\n",
       "      <td>Complete secondary</td>\n",
       "      <td>Complete secondary</td>\n",
       "      <td>-0.267104</td>\n",
       "      <td>0.993806</td>\n",
       "      <td>1.578177</td>\n",
       "      <td>-0.373470</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>975856179</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>not Hispanic or Latino</td>\n",
       "      <td>15.67</td>\n",
       "      <td>Right</td>\n",
       "      <td>4th Grade</td>\n",
       "      <td>Complete primary</td>\n",
       "      <td>Complete secondary</td>\n",
       "      <td>-0.462914</td>\n",
       "      <td>-1.026645</td>\n",
       "      <td>-0.582212</td>\n",
       "      <td>1.333857</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>984757368</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.416667</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>not Hispanic or Latino</td>\n",
       "      <td>16.66</td>\n",
       "      <td>Right</td>\n",
       "      <td>5th Grade</td>\n",
       "      <td>Complete primary</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.467279</td>\n",
       "      <td>0.360029</td>\n",
       "      <td>-0.515655</td>\n",
       "      <td>1.509584</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>987544292</td>\n",
       "      <td>PNC</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>PNC1</td>\n",
       "      <td>1</td>\n",
       "      <td>18.416667</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>not Hispanic or Latino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Right</td>\n",
       "      <td>11th Grade</td>\n",
       "      <td>Complete secondary</td>\n",
       "      <td>Complete primary</td>\n",
       "      <td>-0.303585</td>\n",
       "      <td>0.399735</td>\n",
       "      <td>-0.490472</td>\n",
       "      <td>0.018679</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>534 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     participant_id study study_site session_id  wave        age     sex  \\\n",
       "0        1000881804   PNC       PNC1       PNC1     1  14.916667    Male   \n",
       "1         100527940   PNC       PNC1       PNC1     1   8.250000    Male   \n",
       "2        1006151876   PNC       PNC1       PNC1     1  21.500000  Female   \n",
       "3        1012530688   PNC       PNC1       PNC1     1   8.750000    Male   \n",
       "4        1030193285   PNC       PNC1       PNC1     1  18.000000    Male   \n",
       "..              ...   ...        ...        ...   ...        ...     ...   \n",
       "529       969649154   PNC       PNC1       PNC1     1  12.333333    Male   \n",
       "530       970890500   PNC       PNC1       PNC1     1  18.166667  Female   \n",
       "531       975856179   PNC       PNC1       PNC1     1  11.000000    Male   \n",
       "532       984757368   PNC       PNC1       PNC1     1  13.416667    Male   \n",
       "533       987544292   PNC       PNC1       PNC1     1  18.416667  Female   \n",
       "\n",
       "      race               ethnicity    bmi    handedness participant_education  \\\n",
       "0    Black  not Hispanic or Latino  21.52         Right             7th Grade   \n",
       "1    Black  not Hispanic or Latino    NaN  Ambidextrous             1st Grade   \n",
       "2    Other  not Hispanic or Latino    NaN         Right            12th Grade   \n",
       "3    Black  not Hispanic or Latino  21.36         Right             2nd Grade   \n",
       "4    White  not Hispanic or Latino  22.15         Right            10th Grade   \n",
       "..     ...                     ...    ...           ...                   ...   \n",
       "529  White  not Hispanic or Latino  17.38         Right             5th Grade   \n",
       "530  White  not Hispanic or Latino  30.89         Right            11th Grade   \n",
       "531  White  not Hispanic or Latino  15.67         Right             4th Grade   \n",
       "532  Black  not Hispanic or Latino  16.66         Right             5th Grade   \n",
       "533  White  not Hispanic or Latino    NaN         Right            11th Grade   \n",
       "\n",
       "     parent_1_education  parent_2_education  p_factor  \\\n",
       "0    Complete secondary  Complete secondary -0.429863   \n",
       "1    Complete secondary    Complete primary -0.416456   \n",
       "2     Complete tertiary  Complete secondary -0.309821   \n",
       "3     Complete tertiary  Complete secondary -0.666207   \n",
       "4    Complete secondary    Complete primary -0.292984   \n",
       "..                  ...                 ...       ...   \n",
       "529   Complete tertiary  Complete secondary -0.532445   \n",
       "530  Complete secondary  Complete secondary -0.267104   \n",
       "531    Complete primary  Complete secondary -0.462914   \n",
       "532    Complete primary                 NaN -0.467279   \n",
       "533  Complete secondary    Complete primary -0.303585   \n",
       "\n",
       "     internalizing_mcelroy_harmonized_all_samples  \\\n",
       "0                                        0.097355   \n",
       "1                                        0.699062   \n",
       "2                                        0.495947   \n",
       "3                                       -0.334835   \n",
       "4                                        1.027404   \n",
       "..                                            ...   \n",
       "529                                     -0.148520   \n",
       "530                                      0.993806   \n",
       "531                                     -1.026645   \n",
       "532                                      0.360029   \n",
       "533                                      0.399735   \n",
       "\n",
       "     externalizing_mcelroy_harmonized_all_samples  \\\n",
       "0                                        0.387355   \n",
       "1                                       -0.781881   \n",
       "2                                        0.806481   \n",
       "3                                        1.277773   \n",
       "4                                       -0.490472   \n",
       "..                                            ...   \n",
       "529                                      0.556444   \n",
       "530                                      1.578177   \n",
       "531                                     -0.582212   \n",
       "532                                     -0.515655   \n",
       "533                                     -0.490472   \n",
       "\n",
       "     attention_mcelroy_harmonized_all_samples  cubids_acquisition_group  \n",
       "0                                   -0.467807                       113  \n",
       "1                                   -0.982040                         3  \n",
       "2                                   -0.832210                         1  \n",
       "3                                    0.161110                         4  \n",
       "4                                    2.014568                         1  \n",
       "..                                        ...                       ...  \n",
       "529                                  0.024228                         1  \n",
       "530                                 -0.373470                         1  \n",
       "531                                  1.333857                         1  \n",
       "532                                  1.509584                       114  \n",
       "533                                  0.018679                         1  \n",
       "\n",
       "[534 rows x 19 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will need the RBCPath type from the rbclib package to load data from the RBC.\n",
    "from rbclib import RBCPath\n",
    "\n",
    "# We'll also want to load some data directly from the filesystem.\n",
    "from pathlib import Path\n",
    "\n",
    "# We'll want to load/process some of the data using pandas and numpy.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# This path refers to the repo github.com:ReproBrainChart/PNC_FreeSurfer;\n",
    "# Subject 1000393599's directory is used as an example.\n",
    "subject_id = 1000393599\n",
    "# To browse the repo, use this link:\n",
    "# https://github.com/ReproBrainChart/PNC_FreeSurfer/tree/main\n",
    "sub_path = RBCPath(f'rbc://PNC_FreeSurfer/freesurfer/sub-{subject_id}')\n",
    "\n",
    "# This path refers to a directory:\n",
    "assert sub_path.is_dir()\n",
    "\n",
    "# Print each file in the directory:\n",
    "for file in sub_path.iterdir():\n",
    "    print(repr(file))\n",
    "\n",
    "# We can construct new paths by using the `/` operator. This is identical to\n",
    "# how paths are constructed in the `pathlib` module.\n",
    "stats_filepath = sub_path / f'sub-{subject_id}_regionsurfacestats.tsv'\n",
    "\n",
    "# Use pandas to read in the TSV file then display it:\n",
    "\n",
    "print(f\"Loading {stats_filepath} ...\")\n",
    "with stats_filepath.open('r') as f:\n",
    "    data = pd.read_csv(f, sep='\\t')\n",
    "\n",
    "data\n",
    "\n",
    "# Participant meta-data is generally located in the BIDS repository for each\n",
    "# study:\n",
    "rbcdata_path = Path('/home/jovyan/shared/data/RBC')\n",
    "train_filepath = rbcdata_path / 'train_participants.tsv'\n",
    "test_filepath = rbcdata_path / 'test_participants.tsv'\n",
    "\n",
    "# Load the PNC participants TSV files...\n",
    "with train_filepath.open('r') as f:\n",
    "    train_data = pd.read_csv(f, sep='\\t')\n",
    "with test_filepath.open('r') as f:\n",
    "    test_data = pd.read_csv(f, sep='\\t')\n",
    "\n",
    "# We can also concatenate the two datasets into a single dataset of all\n",
    "# study participants:\n",
    "all_data = pd.concat([train_data, test_data])\n",
    "\n",
    "# Display the full dataframe:\n",
    "all_data\n",
    "\n",
    "#step 1\n",
    "def load_fsdata(participant_id, local_cache_dir=(Path.home() / 'cache')):\n",
    "    \"Loads and returns the dataframe of a PNC participant's FreeSurfer data.\"\n",
    "\n",
    "    # Check that the local_cache_dir exists and make it if it doesn't.\n",
    "    if local_cache_dir is not None:\n",
    "        local_cache_dir = Path(local_cache_dir)\n",
    "        local_cache_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Make the RBCPath and find the appropriate file:\n",
    "    pnc_freesurfer_path = RBCPath(\n",
    "        'rbc://PNC_FreeSurfer/freesurfer',\n",
    "        # We provide the local_cache_dir to the RBCPath object; all paths made\n",
    "        # from this object will use the same cache directory.\n",
    "        local_cache_dir=local_cache_dir)\n",
    "    participant_path = pnc_freesurfer_path / f'sub-{participant_id}'\n",
    "    tsv_path = participant_path / f'sub-{participant_id}_regionsurfacestats.tsv'\n",
    "\n",
    "    # Use pandas to read in the TSV file:\n",
    "    with tsv_path.open('r') as f:\n",
    "        data = pd.read_csv(f, sep='\\t')\n",
    "\n",
    "    # Return the loaded data:\n",
    "    return data\n",
    "\n",
    "\n",
    "example_participant_id = 1000393599\n",
    "data = load_fsdata(example_participant_id)\n",
    "\n",
    "# Display the dataframe we loaded:\n",
    "data\n",
    "\n",
    "row_mask = (data['StructName'] == 'Brodmann.1')\n",
    "data[row_mask]\n",
    "\n",
    "ba1_surfareas = data.loc[row_mask, 'SurfArea']\n",
    "ba1_surfarea = sum(ba1_surfareas)\n",
    "\n",
    "# Show the bilateral surface area for this participant (in square mm):\n",
    "ba1_surfarea\n",
    "\n",
    "def load_ba1_surfarea(participant_id):\n",
    "    \"\"\"Loads and returns the bilateral Brodmann Area 1 surface area for a PNC\n",
    "    study participant.\n",
    "    \"\"\"\n",
    "    # First, load the subject's FreeSurfer dataframe:\n",
    "    data = load_fsdata(participant_id)\n",
    "    # Next, find the relevant rows:\n",
    "    row_mask = (data['StructName'] == 'Brodmann.1')\n",
    "    # Then extract and sum the surface areas:\n",
    "    ba1_surfareas = data.loc[row_mask, 'SurfArea']\n",
    "    ba1_surfarea = sum(ba1_surfareas)\n",
    "    # And return this value:\n",
    "    return ba1_surfarea\n",
    "\n",
    "load_ba1_surfarea(example_participant_id)\n",
    "\n",
    "# First load in surface area data for each participant:\n",
    "print(\"Loading surface areas...\")     \n",
    "\n",
    "# We will put the rows in this dictionary of lists as we build the dataframe:\n",
    "all_vars = {\n",
    "    'participant_id': [],\n",
    "    'ba1_surface_area': [],\n",
    "    'p_factor': []}\n",
    "\n",
    "# We'll display a progress bar `prog` as we go also:\n",
    "from ipywidgets import IntProgress\n",
    "prog = IntProgress(min=0, max=len(all_data))\n",
    "display(prog)\n",
    "\n",
    "# Okay, loop through each row of the `all_data` dataframe, which contains both\n",
    "# training and test subjects, load their BA1 data, and store it in the\n",
    "# all_vars dictionary.\n",
    "for (ii, row) in all_data.iterrows():\n",
    "    # Extract the participant ID and p_factor (which will be NaN for test\n",
    "    # participants).\n",
    "    participant_id = row['participant_id']\n",
    "    p_factor = row['p_factor']\n",
    "    # Load the surface area for this participant:\n",
    "    try:\n",
    "        surf_area = load_ba1_surfarea(participant_id)\n",
    "    except FileNotFoundError:\n",
    "        # Some subjects are just missing the file, so we code them as NaN.\n",
    "        surf_area = np.nan\n",
    "    # Append the participant ID and their surface area to our dataset:\n",
    "    all_vars['participant_id'].append(participant_id)\n",
    "    all_vars['ba1_surface_area'].append(surf_area)\n",
    "    all_vars['p_factor'].append(p_factor)\n",
    "    # Increment the progress bar counter:\n",
    "    prog.value += 1\n",
    "\n",
    "# Convert train_vars into a dataframe.\n",
    "all_vars = pd.DataFrame(all_vars)\n",
    "\n",
    "# Extract the training and test subjects into separate dataframes; the test\n",
    "# participants can be identified as those having NaN values for their\n",
    "# p_factor column.\n",
    "train_vars = all_vars[~np.isnan(all_vars['p_factor'])]\n",
    "test_vars = all_vars[np.isnan(all_vars['p_factor'])]\n",
    "\n",
    "# Display the finished dataframe.\n",
    "all_vars\n",
    "\n",
    "#regression\n",
    "# Import the LinearRegression type:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# LinearRegression requires a matrix whose columns are the variables and whose\n",
    "# final column is the value being predicted (the p_factor for us). We can\n",
    "# extract these columns straight from the dataframes we generated.\n",
    "train_matrix = train_vars.loc[:, ['ba1_surface_area', 'p_factor']].values\n",
    "# We need to exclude rows with NaNs for training:\n",
    "train_okrows = np.all(~np.isnan(train_matrix), axis=1)\n",
    "train_matrix = train_matrix[train_okrows]\n",
    "\n",
    "# Train the regression using the training matrix:\n",
    "lreg = LinearRegression()\n",
    "lreg.fit(train_matrix[:, :1], train_matrix[:, 1])\n",
    "\n",
    "# Display the trained regression parameters:\n",
    "print(\"Linear Regression:\")\n",
    "print(\"  Intercept:\", lreg.intercept_)\n",
    "print(\"  Slope:\", lreg.coef_)\n",
    "\n",
    "#p factor\n",
    "# We can apply the trained linear regression object `lreg` to the 1-column\n",
    "# matrix of ba1_surface_area values in the test_vars dataframe.\n",
    "test_matrix = test_vars.loc[:, ['ba1_surface_area']].values\n",
    "test_okrows = np.all(~np.isnan(test_matrix), axis=1)\n",
    "test_matrix = test_matrix[test_okrows]\n",
    "\n",
    "# Apply the model:\n",
    "p_factor_predictions = lreg.predict(test_matrix)\n",
    "\n",
    "# Display the predictions:\n",
    "p_factor_predictions\n",
    "\n",
    "#save and commit\n",
    "test_data.loc[test_okrows, 'p_factor'] = p_factor_predictions\n",
    "\n",
    "# Display the resulting test data:\n",
    "test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efe416c2-3147-4b91-af2a-4b8f1ca8aa36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLM Results:\n",
      "                  Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:               p_factor   No. Observations:                 1060\n",
      "Model:                            GLM   Df Residuals:                     1058\n",
      "Model Family:                Gaussian   Df Model:                            1\n",
      "Link Function:               Identity   Scale:                         0.85540\n",
      "Method:                          IRLS   Log-Likelihood:                -1420.3\n",
      "Date:                Fri, 01 Aug 2025   Deviance:                       905.01\n",
      "Time:                        19:22:33   Pearson chi2:                     905.\n",
      "No. Iterations:                     3   Pseudo R-squ. (CS):            0.01390\n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.4610      0.028    -16.229      0.000      -0.517      -0.405\n",
      "x1            -0.1094      0.028     -3.851      0.000      -0.165      -0.054\n",
      "==============================================================================\n",
      "Quantile Regression Results:\n",
      "                          QuantReg Regression Results                          \n",
      "==============================================================================\n",
      "Dep. Variable:               p_factor   Pseudo R-squared:             0.009654\n",
      "Model:                       QuantReg   Bandwidth:                      0.4488\n",
      "Method:                 Least Squares   Sparsity:                        2.813\n",
      "Date:                Fri, 01 Aug 2025   No. Observations:                 1060\n",
      "Time:                        19:22:33   Df Residuals:                     1058\n",
      "                                        Df Model:                            1\n",
      "===========================================================================================\n",
      "                              coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------\n",
      "Intercept                  -0.5431      0.043    -12.570      0.000      -0.628      -0.458\n",
      "ba1_surface_area_scaled    -0.1138      0.043     -2.635      0.009      -0.199      -0.029\n",
      "===========================================================================================\n",
      "Slope comparison in raw mm² units:\n",
      "Original Linear Regression slope:   -0.0003118\n",
      "GLM slope (converted to raw units): -0.00031165187081793114\n",
      "Quantile slope (converted to raw):  -0.00032424108661522743\n",
      "Saved GLM + Quantile Regression predictions to results/glm_quantile_comparison.tsv\n"
     ]
    }
   ],
   "source": [
    "# NEW. PERFORMING QUANTILE REGRESSION AND GENERAL LINEAR MODEL TO SEE ANY DIFFERENCE\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools import add_constant\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Standardize BA1 surface area in training and test\n",
    "# -----------------------------\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Drop NaNs from training for fitting scaler\n",
    "train_mask = (~train_vars['ba1_surface_area'].isna()) & (~train_vars['p_factor'].isna())\n",
    "X_train_raw = train_vars.loc[train_mask, ['ba1_surface_area']]\n",
    "y_train = train_vars.loc[train_mask, 'p_factor']\n",
    "\n",
    "# Fit scaler on training BA1\n",
    "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "\n",
    "# Apply scaling to test set (where BA1 is not NaN)\n",
    "test_mask = ~test_vars['ba1_surface_area'].isna()\n",
    "X_test_raw = test_vars.loc[test_mask, ['ba1_surface_area']]\n",
    "X_test_scaled = scaler.transform(X_test_raw)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. GLM (Gaussian)\n",
    "# -----------------------------\n",
    "X_train_glm = add_constant(X_train_scaled, has_constant='add')\n",
    "glm_model = sm.GLM(y_train, X_train_glm, family=sm.families.Gaussian())\n",
    "glm_results = glm_model.fit()\n",
    "print(\"GLM Results:\\n\", glm_results.summary())\n",
    "\n",
    "# Predict on test\n",
    "X_test_glm = add_constant(X_test_scaled, has_constant='add')\n",
    "glm_predictions = glm_results.predict(X_test_glm)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Quantile Regression (Median)\n",
    "# -----------------------------\n",
    "train_df_scaled = pd.DataFrame({\n",
    "    'p_factor': y_train,\n",
    "    'ba1_surface_area_scaled': X_train_scaled.flatten()\n",
    "})\n",
    "\n",
    "quant_model = smf.quantreg('p_factor ~ ba1_surface_area_scaled', train_df_scaled)\n",
    "quant_results = quant_model.fit(q=0.5)\n",
    "print(\"Quantile Regression Results:\\n\", quant_results.summary())\n",
    "\n",
    "# Predict on test\n",
    "test_df_scaled = pd.DataFrame({\n",
    "    'ba1_surface_area_scaled': X_test_scaled.flatten()\n",
    "})\n",
    "quant_predictions = quant_results.predict(test_df_scaled)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Convert slopes back to raw mm² units\n",
    "# -----------------------------\n",
    "ba1_std = train_vars['ba1_surface_area'].std(skipna=True)\n",
    "\n",
    "glm_slope_scaled = glm_results.params.iloc[1]  # slope in z-score units\n",
    "quant_slope_scaled = quant_results.params['ba1_surface_area_scaled']  # slope in z-score units\n",
    "\n",
    "glm_slope_raw = glm_slope_scaled / ba1_std\n",
    "quant_slope_raw = quant_slope_scaled / ba1_std\n",
    "\n",
    "print(\"Slope comparison in raw mm² units:\")\n",
    "print(f\"Original Linear Regression slope:   {-0.0003118}\")\n",
    "print(f\"GLM slope (converted to raw units): {glm_slope_raw}\")\n",
    "print(f\"Quantile slope (converted to raw):  {quant_slope_raw}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Merge predictions into test_data (safe merge)\n",
    "# -----------------------------\n",
    "# Drop old prediction columns if they exist\n",
    "test_data = test_data.drop(columns=['p_factor_glm', 'p_factor_quantile'], errors='ignore')\n",
    "\n",
    "# Create prediction DataFrame\n",
    "pred_df = pd.DataFrame({\n",
    "    'participant_id': test_vars.loc[test_mask, 'participant_id'].values,\n",
    "    'p_factor_glm': glm_predictions,\n",
    "    'p_factor_quantile': quant_predictions.values\n",
    "})\n",
    "\n",
    "# Merge on participant_id\n",
    "test_data = test_data.merge(pred_df, on='participant_id', how='left')\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Save results\n",
    "# -----------------------------\n",
    "Path(\"results\").mkdir(parents=True, exist_ok=True)\n",
    "output_file = \"results/glm_quantile_comparison.tsv\"\n",
    "test_data.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "print(f\"Saved GLM + Quantile Regression predictions to {output_file}\")\n",
    "#TECHNICALLY DON'T SEE NAY DIFFERENCE IN THE RESULTS NETWEEN THE 3 OF THEM.\n",
    "#THERE IS A WEAK NEGETIVE relationship between BA1 SA and p factor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09d4265-a7c4-46c9-a1e4-19abf26f85a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
